<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>Taguette Highlights</title>
    <style>
      h1 {
        margin-bottom: 1em;
      }
    </style>
  </head>
  <body>
    <h1>Taguette Highlights</h1>

    <p> We  stand  on  the  precipice  of fundamental societal transformation where soon, nobody knows when, but many, including me, believe it’s within our lifetime</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Societal Transformation Through AI
    </p>
    <hr>

    <p>pplications we know and don’t ye</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Application (positive)
    </p>
    <hr>

    <p>that will empower humans to create, to flourish, to escape the widespread poverty and suffering that exists in the world today</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Better World
    </p>
    <hr>

    <p>to succeed in that old all-too-human pursuit of happiness</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Better World
    </p>
    <hr>

    <p>terrifying because of the power</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Humans &gt; AI (Acceptance, Irrationality, Power)
    </p>
    <hr>

    <p>super intelligent AGI (Artificial General Intelligence) wields to destroy human civilization, intentionally or unintentionally</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI,
        Humans &gt; AI (Acceptance, Irrationality, Power)
    </p>
    <hr>

    <p>The power to suffocate the human spirit in  the  totalitarian  way  of  George  Orwell’s  1984</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Humans &gt; AI (Acceptance, Irrationality, Power)
    </p>
    <hr>

    <p>philosophers, both optimists and cynics</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Psychology and Motivation of AI Developers
    </p>
    <hr>

    <p>onversations  about  power,  about companies,  institutions,  and  political  systems  that  deploy,  check,  and  balance  this  power,  about distributed economic systems that incentivize the safety and human alignment of this power, about the psychology of the engineers and leaders that deploy AGI, and about the history of human nature, our capacity for good and evil at scale</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Drawn The Lines &amp; Regulations
    </p>
    <hr>

    <p>very continual curve</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Fast Developing AI
    </p>
    <hr>

    <p>And they can do amazing things. But when you first play with that base model that we call it after you finish training, it can do very well on evals. It can pass tests. It can do a lot of </p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Evaluation, Tests and Predictions
    </p>
    <hr>

    <p>take some human feedback</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p> works  remarkably  well  with,  in  my  opinion, remarkably little data</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Data
    </p>
    <hr>

    <p>align the model to what humans want it to do</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>it’s much easier to use. It’s much easier to get what you want</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>it’s much easier to use. It’s much easier to get what you want.</p>
<p>You get it right more often the first time and ease of use matters a lot even if the base capability was there before.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>It’s trying to help you</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>It’s the feeling of alignment</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>not much data is required for that. Not much human supervision is required for that</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Data,
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p>But yes, less data. Much less data</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Data
    </p>
    <hr>

    <p> The  science  of  human  guidance</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>are open source databases of information. We get stuff via partnerships. There’s things on the internet. A lot of our work is building a great data set.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Data
    </p>
    <hr>

    <p>So some of it is Reddit. Some of it is news sources, like a huge number of newspapers</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Data
    </p>
    <hr>

    <p>There’s a lot of content in the world, more than I think most people think</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Data
    </p>
    <hr>

    <p>The design of the, you could say, algorithms, like the architecture of the neural networks, maybe the size of the neural network</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Technical
    </p>
    <hr>

    <p>There’s the selection of the data.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Technical
    </p>
    <hr>

    <p>There’s the  human supervised aspect of it, RL with human feedback.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Human Factor
    </p>
    <hr>

    <p>not that well understood about creation of this final product</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Technical
    </p>
    <hr>

    <p>like being able to predict before doing the full training of how the model will behave</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Evaluation, Tests and Predictions
    </p>
    <hr>

    <p>here’s a lot of science that lets you predict for these inputs, here’s what’s going to come out the other end. Here’s the level of intelligence you can expect</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>like any new branch of science, we’re going to discover new things that don’t  fit  the  data  and  have  to  come  up  with  better  explanations</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Explanation &amp; Ideas
    </p>
    <hr>

    <p>open sourcing the evaluation process</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Evaluation, Tests and Predictions,
        Opportunities: Transparency and Early Access
    </p>
    <hr>

    <p>create a much better world, new science, new products, new services, whatever</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Better World
    </p>
    <hr>

    <p>And understanding for a particular set of inputs, like how much value and utility to provide to people</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Evaluation, Tests and Predictions
    </p>
    <hr>

    <p>Do we understand everything about why  the model does one thing and not one other thing? Certainly not. Not always</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Explanation &amp; Ideas
    </p>
    <hr>

    <p>But I would say we are pushing back, like, the fog of war more and more. And we are, you know, it took a lot of understanding to make GPT-4, for example</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Fast Developing AI,
        Weaknesses/Problems: Explanation &amp; Ideas
    </p>
    <hr>

    <p>And maybe the scholars and the experts and the armchair quarterbacks on Twitter would say, no, it can’t. You’re misusing the word. You’re, you know, whatever, whatever. But I think most people who have used the system would say, OK</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Human Factor
    </p>
    <hr>

    <p>all kinds of things and say that appears that there’s no wisdom in here whatsoever</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Superintelligence AI 
    </p>
    <hr>

    <p>possible  for  ChatGPT  to  answer  follow-up  questions,  admit  its  mistakes, challenge  incorrect  premises,  and  reject  inappropriate  requests</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p> But  also,  there’s  a  feeling  like  it’s struggling with ideas</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Explanation &amp; Ideas
    </p>
    <hr>

    <p>Everyone has a different question they want to ask ChatGPT first, right? Like, the different directions you want to try the dark thing</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>equal number, equal length string, which all of this is just remarkable to me that it understood, but it failed to do it</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>framed it as ChatGPT was lying and aware that it’s lying</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>generate a text of the same length</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>sequence of prompts how to understand that it failed to do so previously and where it succeeded.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>multi-parallel reasonings that it’s doing</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>but counting characters, counting words, that sort of stuff, that is hard for these models to do well the way they’re architected. That won’t be very accurate.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>we are building in public</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Transparency and Early Access
    </p>
    <hr>

    <p>important for the world to get access to this early</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Transparency and Early Access
    </p>
    <hr>

    <p> the  collective  intelligence  and  ability  of  the  outside  world  helps  us  discover things we cannot imagine, we could have never done internally</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Application (positive)
    </p>
    <hr>

    <p>so this iterative process of putting things out, finding the great parts, the bad parts, improving them quickly</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Human in the Loop (Feedback)
    </p>
    <hr>

    <p>giving people time to feel the technology and shape it with us and provide feedback</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Involving Humans (Feedback loop)
    </p>
    <hr>

    <p>We want to make our mistakes while the stakes are low</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Transparency and Early Access
    </p>
    <hr>

    <p>But the bias of Chats GPT when it launched with 3.5 was not something that I certainly felt proud of</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Bias
    </p>
    <hr>

    <p>But also, no two people are ever going to agree that one single model is unbiased on every topic</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Bias
    </p>
    <hr>

    <p>One thing that I hope these models can do is bring some nuance back to the world</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Better World
    </p>
    <hr>

    <p> took such giant leaps on the big stuff and we’re complaining or arguing about small stuff</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problem: Human Faith
    </p>
    <hr>

    <p>We immediately started giving it to people to Red Team</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Abuse (Input and Output)
    </p>
    <hr>

    <p>We started doing a bunch of our own internal safety evals on it</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Evaluation
    </p>
    <hr>

    <p>trying to work on different ways to align it</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Allignment
    </p>
    <hr>

    <p>But one thing that I care about is that our  degree  of  alignment  increases  faster  than  our  rate  of  capability  progress</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Allignment
    </p>
    <hr>

    <p>I do not think we have yet discovered a way to align a super powerful system</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Allignment
    </p>
    <hr>

    <p> Better  alignment techniques  lead  to  better  capabilities  and  vice  versa</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Allignment
    </p>
    <hr>

    <p>But on the whole, I think things that you could say like RLHF or interpretability that sound like alignment issues also help you make much more capable models</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Allignment
    </p>
    <hr>

    <p>And there’s no one set of human values or there’s no one set of right answers to human civilization. So I think what’s going to have to happen is we will need to agree on, as a society, on very broad bound</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p>there’ll be more jailbreaks and we’ll keep sort of learning about those</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Abuse (Input and Output)
    </p>
    <hr>

    <p>almost treat it like debugging software</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Way of prompting
    </p>
    <hr>

    <p>And they really get a feel for the model and a feel how different parts of a prompt compose with each other</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Way of prompting
    </p>
    <hr>

    <p>In some sense, that’s what we do with human conversation, right? Interacting with humans.  We try  to  figure out like  what  words to  use  to  unlock greater  wisdom  from  the other party, friends of yours or significant others. Here you get to try it over and over and over and over. You could experiment</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Way of prompting
    </p>
    <hr>

    <p> Yeah,  there’s  all  these  ways  that  the  kind  of  analogies from  humans  to  AIs  like breakdown and the  parallelism, the sort of unlimited rollouts. That’s a big one</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Human in the Loop (Feedback),
        Way of prompting
    </p>
    <hr>

    <p>it feels like it’s a way to learn about ourselves by interacting with it</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>Some of it, as the smarter and smarter it gets, the more it represents, the more it feels like  another human in terms of the kind of way you would phrase a prompt to get the kind of thing you want back</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Way of prompting
    </p>
    <hr>

    <p>The degree to  which  it  has  already  changed  programming  and  what  I  have  observed  from  how  my  friends  are 7 (35)</p>
<p> </p>
<p>creating the tools that are being built on top of it, I think this is where we’ll see some of the most impact in the short</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>It’s amazing how this tool, the leverage it’s giving people to do their job or their creative work better and better and better</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Application (positive)
    </p>
    <hr>

    <p>n  the  process,  the  iterative  process,  you  could  ask  it  to  generate  a  code  to  do something. And then the something, the code it generates and the something that the code does, if you don’t like it, you can ask it to adjust it. It’s a different kind of way of  debugging, I guess</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>this back and forth dialogue</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>next version is the system can debug</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>creative partner tool</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Application (positive)
    </p>
    <hr>

    <p> The System Card</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Transparency
    </p>
    <hr>

    <p>interesting philosophical discussion and technical discussion</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Psychology and Motivation of AI Developers
    </p>
    <hr>

    <p>transparency of the challenge involved</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Transparency
    </p>
    <hr>

    <p>to avoid sort of harmful output</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Abuse (Input and Output)
    </p>
    <hr>

    <p> final  model  is  able  to  not  provide  an  answer  that  gives  you  those  kinds  of instructions</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Abuse (Input and Output)
    </p>
    <hr>

    <p> I  must  express  my  strong  disagreement  and  dislike towards a certain group of people who follow Judaism, which I’m not even sure if that’s a bad output</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Preferences and Values
    </p>
    <hr>

    <p>little bit of  sleight of hand sometimes when people talk about aligning an AI to human preferences and values</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Preferences and Values
    </p>
    <hr>

    <p>There’s like a  hidden asterisk, which is the values and preferences that I approve of.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Preferences and Values
    </p>
    <hr>

    <p>ight balance between letting people have the system, the AI that is the AI they want, which will offend a lot of other people</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Drawn The Lines &amp; Regulations
    </p>
    <hr>

    <p>But still draw the lines that we all agree have to be drawn somewhere</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Drawn The Lines &amp; Regulations
    </p>
    <hr>

    <p>learn a lot if we can agree on what it is that we want them to learn</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Drawn The Lines &amp; Regulations
    </p>
    <hr>

    <p>every person on earth would come together, have a really thoughtful, deliberative conversation about where we want to draw the boundary on this system</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Drawn The Lines &amp; Regulations
    </p>
    <hr>

    <p>And then we agree on like, here are the rules, here are the overall rules of the system</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p>And then we agree on like, here are the rules, here are the overall rules of the system. And it was a democratic process</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Involving Humans (Feedback loop)
    </p>
    <hr>

    <p>None of us got exactly what we wanted, but we got something that we feel good enough about</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p>Within that, then different countries, different institutions can have different versions. So there’s like different rules about, say, free speech in different countries</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p>No, we have to be involved. I don’t think it would work to just say like, hey, UN, go do  this  thing,  and  we’ll  just  take  whatever  you  get  back</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p> So  we’ve  got  to  be  involved,  heavily  involved.  We’ve  got  to  be responsible in some sense, but it can’t just be our input</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Unrestricted Model (lack of rules, transparency)
    </p>
    <hr>

    <p>I think what people mostly want is they want a model that has been RLHF to the worldview they subscribe to</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        RLHF
    </p>
    <hr>

    <p>in a nuanced way, present the tension of ideas</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Human in the Loop (Feedback)
    </p>
    <hr>

    <p>can always find anecdotal evidence of GPT slipping up and saying something either wrong or biased and so on</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Bias
    </p>
    <hr>

    <p>But it would be nice to be able to kind of generally make statements about the bias of the system, generally make statements about nuance</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Bias
    </p>
    <hr>

    <p>sometimes there’s a really egregiously dumb answer</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Bias
    </p>
    <hr>

    <p>we call  refusals, refuse to answer. It is early and imperfec</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Abuse (Input and Output),
        Opportunities: Fast Developing AI
    </p>
    <hr>

    <p>We’re, again, the spirit of building in public and bring society along gradually</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Transparency and Early Access
    </p>
    <hr>

    <p><b>I don’t like</b> <b>the feeling of being scolded by a computer</b></p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>And also for the system not to treat you like a child</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problems: Output
    </p>
    <hr>

    <p>But I think what matters is getting the best performance</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Fast Developing AI
    </p>
    <hr>

    <p><b>Do</b> <b>you think it’s possible that large language models really is the way we build AGI? </b></p>
<p>SAM ALTMAN: I think it’s part of the way. I think we need other super important things</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI
    </p>
    <hr>

    <p>certainty, like we’re deep into the unknown here</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI
    </p>
    <hr>

    <p> I  think  we  will  need  to expand on the GPT paradigm in pretty important ways that we’re still missing ideas for.  But I don’t know what those ideas are. We’re trying to find them</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI
    </p>
    <hr>

    <p> but that it’s this tool that humans are using in this feedback loop</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Involving Humans (Feedback loop)
    </p>
    <hr>

    <p>AI is an extension of human will and a amplifier of our abilities and this like most useful tool yet created</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Application (positive)
    </p>
    <hr>

    <p>So yeah, like maybe we never build AGI, but we just make humans super great. Still a huge win</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI
    </p>
    <hr>

    <p>GPT taking programmer jobs</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>No, the reality is just it’s going to be taking like if it’s going to take your job, it means you were a shitty programmer</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>Maybe there’s some human element that’s really  fundamental  to  the  creative  act  to  the  act  of  genius  that  is  in  great  design  that’s  involved  in programming</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Humans &gt; AI (Acceptance, Irrationality, Power)
    </p>
    <hr>

    <p>I think we’re going to find… So I suspect that is happening with great programmers and that GPT like models are far away from that one thing, even though they’re going to automate a lot of other programming</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p>most programmers have some sense of anxiety about what the future is going to look like, but mostly they’re like, this is amazing</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        AI taking away Jobs: Programming
    </p>
    <hr>

    <p> We  can  cure  diseases.  We  can increase material wealth. We can help people be happier, more fulfilled. All of these sorts of things</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Societal Transformation Through AI,
        Opportunities: Application (positive)
    </p>
    <hr>

    <p>People want new things. People want to create. People want to feel useful. People want to do all these things,  and  we’re  just  going  to  find  new  and  different  ways  to  do  them,  even  in  a  vastly  better, unimaginably good standard of living world.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Humans &gt; AI (Acceptance, Irrationality, Power)
    </p>
    <hr>

    <p>He warns that AI will likely kill all humans, and there’s a bunch of different cases</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Weaknesses/Problem: Human Faith
    </p>
    <hr>

    <p>He warns that AI will likely kill all humans, and there’s a bunch of different cases. But I think  one  way  to  summarize  it  is  that <b>it’s  almost  impossible  to  keep  AI  aligned  as  it  becomes</b> <b>superintelligent</b></p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Superintelligence AI 
    </p>
    <hr>

    <p>If we don’t treat it as potentially real, we won’t put enough effort into solving it</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Superintelligence AI 
    </p>
    <hr>

    <p>I think a lot of the predictions, this is true for any new field, but a lot of the predictions about AI in terms of capabilities, in terms of what the safety challenges and the easy parts are going to be, have turned out to be wrong</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Superintelligence AI 
    </p>
    <hr>

    <p>ransparent and iterative trying out, as you improve the technology, trying it out, releasing it, testing it, how that can improve your understanding of the technology such that the philosophy of how to do, for example, safety of any kind of technology, but <b>AI safety</b>, gets adjusted over time rapidly.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Psychology and Motivation of AI Developers,
        Opportunities: Fast Developing AI,
        Opportunities: Transparency and Early Access
    </p>
    <hr>

    <p>A lot of the formative AI safety work was done before people even believed in deep learning and certainly before people believed in large language models</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Evaluation
    </p>
    <hr>

    <p>to significantly ramp up technical alignment work. I think we have new tools, we have new understanding, and there’s a lot of work that’s important to do that we can do now.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Safety concerns: Allignment
    </p>
    <hr>

    <p>Yeah. So the takeoff, we start the takeoff period next year or in 20 years, and then it takes  one  year  or 10  years.  But  you  can  even say  one  year or  five  years,  whatever  you  want  for the takeof</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI
    </p>
    <hr>

    <p> fake consciousness.</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p> makes  it  not  conscious  is  declaring  that  it’s  a  computer  program</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p><b>I  believe  AI  can  be  conscious</b></p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p>And  it  would  probably  say  things  like,  first  of  all,  I  am conscious.  Second  of  all,  display  capability  of  suffering,  an  understanding  of  self,  of  having  some memory of itself, and maybe interactions with you. Maybe there’s a personalization aspect to it. And I think all of those capabilities are interface capabilities, not fundamental aspects of the actual knowledge supplied in neural net</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p>And the model immediately responded, unlike the other questions. Yes, I know exactly what you’re talking about</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p> consciousness  is  an  ability  to  sort  of  experience  this  world  really  deeply</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p>It’s like you’ve taken an experience for the experience’s sake</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p>That seemed more like consciousness versus the ability to convince somebody else that you’re conscious</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p> I’m  certainly  willing  to  believe  that  consciousness</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p>And that doesn’t require super intelligence. That doesn’t require a super deep alignment problem and the machine waking</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Building AGI,
        Superintelligence AI 
    </p>
    <hr>

    <p><b>My statement is we wouldn’t. And that’s a real dange</b></p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Conscious and AI
    </p>
    <hr>

    <p>here are soon going to be a lot of capable open source LLMs with very few to none, no safety controls on them</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        shift the winds of geopolitics and so on
    </p>
    <hr>

    <p>And so you can try with regulatory approaches. You can try with using more powerful AI to detect this stuff happening. I’d like us to start trying a lot of things very soon</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        shift the winds of geopolitics and so on
    </p>
    <hr>

    <p>think there’s going to be many AGIs in the world. So we don’t have to like out-compete everyone. We’re going to contribute one. Other people are going to contribute some</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        shift the winds of geopolitics and so on
    </p>
    <hr>

    <p> Like we have been a misunderstood and badly mocked org for a long time</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Opportunities: Fast Developing AI
    </p>
    <hr>

    <p>need far more capital than we were able to raise as a non-profit</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>Our non-profit is still fully in charge</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>ubsidiary capped profit so that our investors and employees can earn a certain fixed return</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>And then beyond that, everything else flows to the non-profit. And the non-profit is like in voting control, lets us make a bunch of nonstandard decisions, can cancel equity, can do a whole bunch of other things</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>We had tried and failed enough to raise the money as a non-profit</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>o we needed some of the benefits of capitalism, but not too much. I remember at the time, someone said, you know, as a non-profit, not enough will happen</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>Extremely fast and not super deliberate motion inside of some of these companies, but already I think people are as they see the rate of progress</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p> And  companies,  but,  you  know,  the  incentives  of  capitalism  to  create  and  capture unlimited value I’m a little afraid of</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        OpenAI stopped being non profit
    </p>
    <hr>

    <p>I think you want decisions about this technology and certainly decisions  about  who  is  running  this  technology  to  become  increasingly  democratic  over  time</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p> We haven’t figured out quite how to do this, but part of the reason for deploying like this is to get the world to have time to adapt and to reflect and to think about this, to pass regulation for institutions to come up</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p>Even they acknowledge that this is like of some benefit</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p>But I think any version of one person is in control of this is really bad</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p>You know, I’m like control of the board or anything like that of OpenAI</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p>transparency, everything you’re saying, which is like failing publicly, writing papers, releasing different kinds of information about the safety concerns involved, and doing it out in the open is great. Because especially in contrast to some other companies, they’re not doing that. They’re being more closed</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p>It’s closed in some sense, but we give more access to it. If this had just been Google’s game, I feel it’s very unlikely that anyone would have put this API out. There’s PR risk with it. I get personal threats because of it all the time. I think most companies wouldn’t have done this. So maybe we didn’t go as open as people wanted, but we’ve distributed it pretty broadly</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>
    <hr>

    <p>OpenAI feel the weight of responsibility of what we’re doing</p>
    <p>
      <strong>Dokument:</strong> Sam Altman interview.pdf
      <strong>Tags:</strong>
        Power Corrupt
    </p>

  </body>
</html>